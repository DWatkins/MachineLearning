{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Decision Tree Classifiers\n",
    "\n",
    "## Overview\n",
    "\n",
    "This decision tree classifier is implemented as an extension of the [anytree](https://anytree.readthedocs.io/en/latest/index.html) module. By inheriting from the any tree node class, the tree structure is easily maintained and all custom code is simply added to the class.\n",
    "\n",
    "Data manipulation is performed using [pandas](https://pandas.pydata.org/), which has many low-level functions built in to perform functions not directly related to the Decision Tree Classifier algorithms that are the focus of the assignment.\n",
    "\n",
    "## Part 1: Splitting Algorithm\n",
    "\n",
    "The splitting algorithm is implemented as follows\n",
    "\n",
    "```\n",
    "if stop criterion have not been met:\n",
    "    compute the optimal split among all features and all split values of each feature\n",
    "    split the samples into two subsamples based on this optimal split\n",
    "    for each sample subset, apply the splitting algorithm\n",
    "```\n",
    "\n",
    "The stopping criterion, as specified, are that either the sample set is pure or all features of the set are identical. Pandas was used to quickly perform both of these checks. Purity was checked by looking at the number of unique labels within the set. Feature similarity was checked by ensuring that more than one sample still existed when duplicate features were removed\n",
    "\n",
    "## Part 2: Pruning Algorithm\n",
    "\n",
    "The pruning algorithm is implemented as follows:\n",
    "\n",
    "```\n",
    "Set the best observed accuracy to 0\n",
    "At the root node:\n",
    "    Make a list of all descendants of the node\n",
    "    For each descendant:\n",
    "        temporarily remove the node and its descendants\n",
    "        calculate the accuracy of the new tree\n",
    "        Add the node back into the tree\n",
    "        if the new accuracy is greater than the current best accuracy:\n",
    "            update best accuracy\n",
    "            track the node that whose removal gave this accuracy\n",
    "    Remove the best node (and its descendants) that gives the best resultant accuracy\n",
    "```\n",
    "Anytree makes this process very simple, since many of these steps simply involve changing a node's parent temporarily and then iterating over the new tree\n",
    "\n",
    "## Part 3: Data Analysis\n",
    "\n",
    "The follow code blocks contain the DCT implementation using pandas and anytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas for dataset manipulation, anytree for tree visualization, and supporting modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os as os\n",
    "import anytree as at\n",
    "import anytree.exporter as texporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTCNode(at.Node):\n",
    "    def __init__(self, name, samples, features,impurityType, resultVal, parent=None):\n",
    "        self.name = name\n",
    "        self.totalNodes = 1\n",
    "        self.totalLeafNodes = 0\n",
    "        self.parent = parent\n",
    "        self.splitFeature = None\n",
    "        self.splitValue = np.nan\n",
    "        self.samples = samples\n",
    "        self.trueSamples = None\n",
    "        self.falseSamples = None\n",
    "        self.features = features\n",
    "        self.resultVal = resultVal\n",
    "        self.label = self.computeLabel()\n",
    "        self.impurityType = impurityType\n",
    "        self.impurity = self.computeImpurity(self.samples)\n",
    "        self.root.totalNodes += 1\n",
    "        \n",
    "        \n",
    "        if not self.stopCriterionIsMet():\n",
    "            [self.splitFeature, self.splitValue, self.trueSamples, self.falseSamples, isLeaf] = self.computeOptimalSplit()\n",
    "            \n",
    "            if isLeaf:\n",
    "                #print(\"I'm a leaf node!\")\n",
    "                #print(\"Leaf Node Samples\", self.samples)\n",
    "                self.root.totalLeafNodes += 1\n",
    "                self.name = self.label\n",
    "            else:\n",
    "                self.name = self.name + \"\\n\" + self.splitFeature + \" <= \" + str(self.splitValue)\n",
    "                #print(\"Splitting samples into child nodes!\")\n",
    "                #if not self.parent == None:\n",
    "                #    print(\"Parent Samples\\n\",self.parent.samples)\n",
    "                #    print(\"Parent Samples Value Count\\n\", self.parent.samples[self.parent.samples.columns[0]].value_counts())\n",
    "                #print(\"Split Feature\\n\",self.splitFeature)\n",
    "                #print(\"Split Value\\n\",self.splitValue)\n",
    "                #print(\"True Samples\\n\", self.trueSamples)\n",
    "                #print(\"False Samples\\n\",self.falseSamples)\n",
    "                DTCNode(\"True\", self.trueSamples, self.features, self.impurityType, True, parent=self)\n",
    "                DTCNode(\"False\", self.falseSamples, self.features, self.impurityType, False, parent=self)\n",
    "        else:\n",
    "            #print(\"I'm a leaf node!\")\n",
    "            #print(\"Leaf Node Samples\", self.samples)\n",
    "            self.root.totalLeafNodes += 1\n",
    "            self.name = self.label\n",
    "        if parent == None:\n",
    "            texporter.DotExporter(self).to_picture('TestTree.png')     \n",
    "            \n",
    "    def stopCriterionIsMet(self):\n",
    "        return self.samplesArePure(self.samples) or self.sampleFeaturesAreIdentical(self.samples)\n",
    "    \n",
    "    # samples are pure if all the labels are the same\n",
    "    # use pandas built-ins to do a bit of the heavy lifting\n",
    "    # by counting the number of unique values within the label column,\n",
    "    # if it is pure, there will only be a single unique value\n",
    "    def samplesArePure(self, samples):\n",
    "        pure = samples[samples.columns[0]].nunique() == 1\n",
    "        return pure\n",
    "    \n",
    "    # again use pandas built-ins for heavy lifting\n",
    "    # by removing all rows with duplicate feature values. \n",
    "    # If all features are identical for all samples,\n",
    "    # only one row will be remaining       \n",
    "    def sampleFeaturesAreIdentical(self, samples):\n",
    "        identical = len(samples.drop_duplicates(self.features).index) == 1\n",
    "        return identical\n",
    "        \n",
    "    # iterate through all features(and possible splits within those features)\n",
    "    # in the node's samples and find the feature/split combo that minimizes the resultant\n",
    "    # child node's weighted impurity.\n",
    "    def computeOptimalSplit(self):\n",
    "        bestImpurity = 1\n",
    "        bestFeature = None\n",
    "        bestValue = None\n",
    "        bestTrueSubSample = None\n",
    "        bestFalseSubSample = None\n",
    "        #iterate through each feature\n",
    "        for feature in self.features:\n",
    "            #create an ordered list of values\n",
    "            orderedValueList = self.samples[feature].sort_values(ascending=True).drop_duplicates()\n",
    "            #create array of potential split values by averaging adjacent values in the list\n",
    "            splitVals = []\n",
    "            orderedValueList=orderedValueList.values\n",
    "            for i in range(1,orderedValueList.size-1):\n",
    "                \n",
    "                #print(i)\n",
    "                #print(orderedValueList[i-1])\n",
    "                #print(orderedValueList[i])\n",
    "                splitVals.append((orderedValueList[i]+orderedValueList[i-1])/2)\n",
    "            for value in splitVals:\n",
    "                [tempTrueSamples, tempFalseSamples] = self.split(feature, value)\n",
    "                tempImpurity = self.computeWeightedImpurity(self.samples, \n",
    "                                                           tempTrueSamples,\n",
    "                                                           tempFalseSamples)\n",
    "                if tempImpurity < bestImpurity:\n",
    "                    #print(\"Found a better split criterion!\")\n",
    "                    #print(bestFeature, bestValue)\n",
    "                    bestImpurity = tempImpurity\n",
    "                    bestFeature = feature;\n",
    "                    bestValue = value\n",
    "                    bestTrueSubSample = tempTrueSamples\n",
    "                    bestFalseSubSample = tempFalseSamples\n",
    "                    #print(bestTrueSubSample)\n",
    "                    #print(bestFalseSubSample)\n",
    "        return [bestFeature, bestValue, bestTrueSubSample, bestFalseSubSample, bestFeature==None]\n",
    "        \n",
    "    # returns two subsamples by splitting a node's samples based on a specified\n",
    "    # feature and value\n",
    "    # Uses pandas for heavy lifting\n",
    "    def split(self, feature, value):\n",
    "        # pandas supports boolean indexing, which makes this pretty trivial\n",
    "        trueSubSamples = self.samples[self.samples[feature] <= value]\n",
    "        falseSubSamples = self.samples[self.samples[feature] > value]\n",
    "        return [trueSubSamples, falseSubSamples]\n",
    "        \n",
    "    def computeImpurity(self, samples):\n",
    "        if self.impurityType == \"Gini\":\n",
    "            return self.giniImpurity(samples)\n",
    "        elif self.impurityType == \"Entropy\":\n",
    "            return self.entropyImpurity(samples)\n",
    "        else:\n",
    "            raise ValueError(\"A valid impurity measure type was not specified!\")\n",
    "     \n",
    "    # calculates the weighted impurity of a node's children\n",
    "    # this function takes many shortcuts due to the assumptions\n",
    "    # that it will only be computing the weighted impurity for \n",
    "    # a group of two nodes (the two children of a parent node)\n",
    "    def computeWeightedImpurity(self, parentSamples, subSamples1, subSamples2):\n",
    "        totalSamples = len(parentSamples.index)\n",
    "        totalSubSamples1 = len(subSamples1.index)\n",
    "        totalSubSamples2 = len(subSamples2.index)\n",
    "        return ((totalSubSamples1/totalSamples)*self.computeImpurity(subSamples1) +\n",
    "                (totalSubSamples2/totalSamples)*self.computeImpurity(subSamples2))\n",
    "            \n",
    "    # calculate the gini index of a set of samples  \n",
    "    # this function takes many shortcuts due to the assumption \n",
    "    # that it will only be computing the gini index of binary data\n",
    "    def giniImpurity(self, samples):\n",
    "        #check if the sample set is pure\n",
    "        if self.samplesArePure(samples):\n",
    "            return 0\n",
    "        #print(samples)\n",
    "        value_counts = samples[samples.columns[0]].value_counts()\n",
    "        labelOneCounts = value_counts[0]\n",
    "        labelTwoCounts = value_counts[1]\n",
    "        totalCounts = labelOneCounts + labelTwoCounts\n",
    "        return 1 - np.square(labelOneCounts/totalCounts) - np.square(labelTwoCounts/totalCounts)\n",
    "        \n",
    "        \n",
    "    # calculate the entropy of a set of samples\n",
    "    # this function takes many shortcuts due to the assumption\n",
    "    # that it will only be computing the gini index of binary data\n",
    "    def entropyImpurity(self, samples):\n",
    "        #check if the sample set is pure\n",
    "        if self.samplesArePure(samples):\n",
    "            return 0\n",
    "        value_counts = samples[samples.columns[0]].value_counts()\n",
    "        labelOneCounts = value_counts[0]\n",
    "        labelTwoCounts = value_counts[1]\n",
    "        totalCounts = labelOneCounts + labelTwoCounts\n",
    "        return (-(labelOneCounts/totalCounts)*np.log2(labelOneCounts/totalCounts) - \n",
    "                (labelTwoCounts/totalCounts)*np.log2(labelTwoCounts/totalCounts))\n",
    "    \n",
    "    def computeLabel(self):\n",
    "        # assign a label to the node that is the label\n",
    "        # with the largest frequency within the nodes samples\n",
    "        # value_counts returns frequency counts in descending order \n",
    "        # by default, so grab the index of the first one\n",
    "        label = self.samples[self.samples.columns[0]].value_counts().index[0]\n",
    "        \n",
    "        return label\n",
    "        \n",
    "    def classify(self, sample):\n",
    "        if self.is_leaf:\n",
    "            return self.label\n",
    "        elif len(self.children) == 1:\n",
    "            return self.children[0].classify(sample)\n",
    "        else:\n",
    "            \n",
    "            if (sample[self.splitFeature].values[0] <= self.splitValue) == self.children[0].resultVal:\n",
    "                return self.children[0].classify(sample)\n",
    "            else:\n",
    "                return self.children[1].classify(sample)\n",
    "    \n",
    "    def performClassification(self, samples, labels):\n",
    "        results = []\n",
    "        for i in range(0, len(samples.index)):\n",
    "            results.append(self.classify(samples.iloc[[i]]))\n",
    "        accurateResults = np.equal(np.asarray(results), np.asarray(labels))\n",
    "        return np.sum(accurateResults)/len(samples.index)\n",
    "    \n",
    "    def pruneSingleGreedyNode(self, validation, testing):\n",
    "        if len(self.children) == 0:\n",
    "            return [self.performClassification(samples), True]\n",
    "        bestValidationAccuracy = 0\n",
    "        bestNode = None\n",
    "        for node in self.descendants:\n",
    "            parent = node.parent\n",
    "            node.parent = None\n",
    "            validationAccuracy = self.performClassification(validation)\n",
    "            node.parent = parent;\n",
    "            if validationAccuracy > bestValidationAccuracy:\n",
    "                bestValidationAccuracy = validationAccuracy\n",
    "                bestNode = node\n",
    "        for node in bestNode.descendants:\n",
    "            self.totalNodes -= 1\n",
    "            if node.is_leaf:\n",
    "                self.totalLeafNodes -= 1\n",
    "        bestNode.parent = None\n",
    "        testAccuracy = self.performClassification(testing)\n",
    "        return [testAccuracy, bestValidationAccuracy, False]\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Cont'd\n",
    "The next step is to read in the datasets. The first run will use dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training dataset to be used\n",
    "training = pd.read_csv('cancer_datasets_v2/training_1.csv')\n",
    "#training.head()\n",
    "trainingLabels = training[training.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of attributes for use later\n",
    "# only need to do this once since all data has the same features\n",
    "features = training.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in validation dataset to be used\n",
    "validation = pd.read_csv('cancer_datasets_v2/validation_1.csv')\n",
    "#validation.head()\n",
    "validationLabels = validation[validation.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test datasets to be used\n",
    "test = pd.read_csv('cancer_datasets_v2/testing_1.csv')\n",
    "#test.head()\n",
    "testLabels = test[test.columns[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis: Training\n",
    "\n",
    "The following block trains the tree on this data, using the **Entropy** impurity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tree\n",
    "t1 = DTCNode(\"Root\", training, features, \"Entropy\",True, parent=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis: Training Results\n",
    "\n",
    "The following code blocks provide the total number of nodes and leaf nodes as well as the classification accuracy on the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nodes 34\n",
      "Total Leaf Nodes 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Nodes\", t1.totalNodes)\n",
    "print(\"Total Leaf Nodes\", t1.totalLeafNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  1.0\n",
      "Test Accuracy:  0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \", t1.performClassification(training, trainingLabels))\n",
    "print(\"Test Accuracy: \", t1.performClassification(test, testLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis: Training with other dataset\n",
    "The same analsysis is performed with the dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in training dataset to be used\n",
    "training = pd.read_csv('cancer_datasets_v2/training_2.csv')\n",
    "#training.head()\n",
    "trainingLabels = training[training.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of attributes for use later\n",
    "# only need to do this once since all data has the same features\n",
    "features = training.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in validation dataset to be used\n",
    "validation = pd.read_csv('cancer_datasets_v2/validation_2.csv')\n",
    "#validation.head()\n",
    "validationLabels = validation[validation.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in test datasets to be used\n",
    "test = pd.read_csv('cancer_datasets_v2/testing_2.csv')\n",
    "#test.head()\n",
    "testLabels = test[test.columns[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the tree\n",
    "t2 = DTCNode(\"Root\", training, features, \"Entropy\",True, parent=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Nodes 30\n",
      "Total Leaf Nodes 15\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Nodes\", t2.totalNodes)\n",
    "print(\"Total Leaf Nodes\", t2.totalLeafNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9978021978021978\n",
      "Test Accuracy:  0.8771929824561403\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: \", t2.performClassification(training, trainingLabels))\n",
    "print(\"Test Accuracy: \", t2.performClassification(test, testLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis: Dataset Comparison\n",
    "\n",
    "As can be see, dataset 1 has a much higher initial accuracy, for both testing and training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis: Pruning\n",
    "Unfortunately I ran out of time before getting to this section. The pruning algorithm is implemented (see code), but I was not able to perform the analysis asked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Visualization\n",
    "The following figure is a graphical representation of a tree trained on **dataset 1** and using **Entropy** as the impurity measure.  When viewing the tree, the **True/False** values at the top of each node represent the result of evaluating its parents feature split. The feature split of each node is then listed.\n",
    "\n",
    "Note that due to the limitations of anytree's utilization of graphviz, all nodes with the same name are treated as a single node. Hence, the visualization only shows two leaf nodes named with the two possible labels, when in fact each arrow pointing to the leaf node is actually a distinct node. This is verified by the count metrics implemented in the code to determine the total number of nodes and leaf nodes (see above)\n",
    "\n",
    "![TreeViz](TreeViz.png)\n",
    "\n",
    "This picture will also be submitted separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
